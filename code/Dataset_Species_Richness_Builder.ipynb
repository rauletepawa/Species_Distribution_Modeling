{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e706a052-11ea-4254-a651-d23738f13ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import box\n",
    "\n",
    "# Load the Norway shapefile\n",
    "norway = gpd.read_file(\"../data/gadm41_NOR_shp/gadm41_NOR_0.shp\")\n",
    "\n",
    "# Reproject to UTM zone 33N (EPSG:32633) for 1x1 km grid creation\n",
    "norway = norway.to_crs(epsg=32633)\n",
    "\n",
    "# Function to create a 1km x 1km grid\n",
    "def create_grid(bounds, cell_size=1000):\n",
    "    xmin, ymin, xmax, ymax = bounds\n",
    "    grid_cells = []\n",
    "    \n",
    "    for x in range(int(xmin), int(xmax), cell_size):\n",
    "        for y in range(int(ymin), int(ymax), cell_size):\n",
    "            grid_cells.append(box(x, y, x + cell_size, y + cell_size))\n",
    "    \n",
    "    return gpd.GeoDataFrame(geometry=grid_cells, crs=\"EPSG:32633\")\n",
    "\n",
    "# Create grid over the bounding box of Norway\n",
    "grid = create_grid(norway.total_bounds)\n",
    "\n",
    "# Clip grid to Norway borders (optional but cleaner)\n",
    "grid = gpd.overlay(grid, norway, how='intersection')\n",
    "\n",
    "# Compute centroids in UTM (projected) coordinates\n",
    "grid[\"centroid\"] = grid.geometry.centroid\n",
    "\n",
    "# Create a new GeoDataFrame with centroids, and convert to WGS84 for lat/lon\n",
    "centroids = grid.copy()\n",
    "centroids = centroids.set_geometry(\"centroid\").to_crs(epsg=4326)\n",
    "\n",
    "# Extract latitude and longitude\n",
    "centroids[\"latitude\"] = centroids.geometry.y\n",
    "centroids[\"longitude\"] = centroids.geometry.x\n",
    "\n",
    "# Final DataFrame with centroid lat/lon and polygon geometry\n",
    "grid[\"latitude\"] = centroids[\"latitude\"]\n",
    "grid[\"longitude\"] = centroids[\"longitude\"]\n",
    "\n",
    "# Keep only needed columns\n",
    "grid_df = grid[[\"geometry\", \"latitude\", \"longitude\"]]\n",
    "\n",
    "# Done: This is your 1km grid with centroids\n",
    "grid_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bce4f34-70c6-4a89-9093-dfb16e53f024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definitive function from Sigma2\n",
    "\n",
    "import geopandas as gdp\n",
    "import apndas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import box\n",
    "\n",
    "norway = gpd.read_file(\"../data/gadm41_NOR_shp/gadm41_NOR_0.shp\")\n",
    "\n",
    "# Reproject to UTM zone 33N (EPSG:32633) for 1x1 km grid creation\n",
    "norway = norway.to_crs(epsg=32633)\n",
    "\n",
    "# simplify geometry (tolerance in meters)\n",
    "norway_simple = norway.copy()\n",
    "norway_simple[\"geometry\"] = norway.simplify(tolerance=500, preserve_topology = True)\n",
    "\n",
    "# 4. Compare original vs simplified\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "norway.plot(ax=ax[0], edgecolor='black')\n",
    "ax[0].set_title(\"Original Norway Geometry\")\n",
    "norway_simple.plot(ax=ax[1], edgecolor='red')\n",
    "ax[1].set_title(\"Simplified Norway Geometry\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Function to create a 1x1 km grid\n",
    "def create_grid(bounds, cell_size=1000):\n",
    "    xmin, ymin, xmax, ymax = bounds\n",
    "    x_coords = np.arange(xmin, xmax, cell_size)\n",
    "    y_coords = np.arange(ymin, ymax, cell_size)\n",
    "\n",
    "    polygons = [box(x, y, x + cell_size, y + cell_size) for x in x_coords for y in y_coords]\n",
    "    return gdp.GeoDataFrame(geometry=polygons, crs=\"EPSG:32633\")\n",
    "\n",
    "# 6. Create a grid and clip it using simplified borders\n",
    "grid = create_grid(norway.total_bounds)\n",
    "print(\"grid created\")\n",
    "grid = gpd.overlay(grid, norway_simple, how = 'intersection') # much faster\n",
    "\n",
    "# 7. Calculate centroids and convert to lat/lon (WGS84)\n",
    "grid[\"centroid\"] = grid.geometry.centroid\n",
    "centroids = grid.set_geometry(\"centroid\").to_crs(epsg = 4326)\n",
    "grid[\"latitude\"] = centroids.geometry.y\n",
    "grid[\"longitude\"] = centroids.geometry.x\n",
    "\n",
    "# 8 keep only necessary info\n",
    "grid_df = grid[[\"geometry\",\"latitude\",\"longitude\"]]\n",
    "\n",
    "grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48c6580-7c7b-4131-857b-54a378910c07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c779fc54-9311-4ea2-8dd3-fa0f7d98072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def read_netcdf_stack(file_paths, latitude, longitude, grid):\n",
    "    patches = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        data = xr.open_dataset(file_path)\n",
    "        variable = list(data.data_vars.keys())[-1]\n",
    "\n",
    "        lat_idx = np.argmin(np.abs(data['lat'].values - latitude))\n",
    "        lon_idx = np.argmin(np.abs(data['lon'].values - longitude))\n",
    "\n",
    "        patch = data[variable].isel(\n",
    "            lat=slice(lat_idx - grid, lat_idx + grid),\n",
    "            lon=slice(lon_idx - grid, lon_idx + grid)\n",
    "        ).fillna(0).values  # Convert to NumPy\n",
    "\n",
    "        print(f\"Shape of patch for {file_path} {variable}: {patch.shape}\")\n",
    "\n",
    "        # Remove extra dimension if necessary\n",
    "        if patch.shape[0] == 1:\n",
    "            patch = patch[0]\n",
    "\n",
    "        patches.append(patch)  # Add raw patch (not scaled)\n",
    "\n",
    "    # Stack along last axis (channels dimension)\n",
    "    stacked_patches = np.stack(patches, axis=-1)\n",
    "\n",
    "    return stacked_patches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda7bb0e-c0cd-44e7-8387-359f567a98d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_altitude = r\"C:\\Users\\Raul\\Desktop\\TFM\\CHELSA\\DEM\\dem_latlong.nc\"\n",
    "\n",
    "variables = ['gdd5', 'prsd', 'bio12d', 'swe', 'bio01d', 'bio04d', 'cdd', 'fd', 'bio15d', 'scd']\n",
    "file_paths = [f\"CHELSA/1991-2020/chelsav2/EUR11/obs/annual/V2.1/{var}/CHELSA_EUR11_obs_{var}_2000_V.2.1.nc\" for var in variables]\n",
    "file_paths.append(path_altitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0744c4-7a13-49db-bc38-5d59eaf6fe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 32 # resolutions = [8,16,32]\n",
    "stacked_patches_list = []\n",
    "#x = 100\n",
    "for i in range(gbif_assemblages.shape[0]): # x\n",
    "    sample =  gbif_assemblages.iloc[i]\n",
    "    file_paths = [f\"CHELSA\\\\1991-2020\\\\chelsav2\\\\EUR11\\\\obs\\\\annual\\\\V2.1\\\\{var}\\\\CHELSA_EUR11_obs_{var}_{sample.year}_V.2.1.nc\" for var in variables]\n",
    "    file_paths.append(path_altitude)\n",
    "        \n",
    "    # Call the function to get the stacked patches\n",
    "    stacked_patches = read_netcdf_stack(file_paths = file_paths, \n",
    "                                            latitude = sample.latitude, \n",
    "                                            longitude = sample.longitude, \n",
    "                                            grid = resolution) # better to use multiples of 64\n",
    "        \n",
    "    stacked_patches_list.append(stacked_patches)\n",
    "    \n",
    "    print(f'Sample {i} correctly stacked and added to the list')\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8465c756-5a93-4c66-b6ff-5b2e7cef8b58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd610c6f-11d3-441e-b4b6-efa6ec302315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame as a Pickle file\n",
    "with open(f'Data\\\\Full_Scale\\\\vectorized__climatic_maps_ALL-NORWAY_2018_{resolution*2}.pkl', \"wb\") as file_2:\n",
    "    pickle.dump(gbif_assemblages_climatic, file_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
